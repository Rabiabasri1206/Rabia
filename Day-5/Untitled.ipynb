{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classification in Python\n",
    "\n",
    "## Topics: \n",
    "* Decision Tree Classification\n",
    "* Attribute selection measures\n",
    "* How to build and optimize Decision Tree Classifier using Python Scikit-learn package.\n",
    "\n",
    "    * Decision Tree Algorithm\n",
    "    * How does the Decision Tree algorithm work?\n",
    "    * Attribute Selection Measures\n",
    "    * Information Gain\n",
    "    * Gain Ratio\n",
    "    * Gini index\n",
    "    * Optimizing Decision Tree Performance\n",
    "    * Classifier Building in Scikit-learn\n",
    "    * Pros and Cons\n",
    "\n",
    "\n",
    "\n",
    "# Business Problem\n",
    "\n",
    "* As a marketing manager, you want a set of customers who are most likely to purchase your product. \n",
    "    * This is how you can save your marketing budget by finding your audience. \n",
    "\n",
    "\n",
    "* As a loan manager, you need to identify risky loan applications to achieve a lower loan default rate. \n",
    "    * This process of classifying customers into a group of potential and non-potential customers or safe or risky loan applications is known as a classification problem. \n",
    "\n",
    "\n",
    "* Classification is a two-step process: \n",
    "    * Learning step\n",
    "        * In the learning step, the model is developed based on given training data. \n",
    "        \n",
    "    * Prediction step \n",
    "        * In the prediction step, the model is used to predict the response for given data.\n",
    "\n",
    "\n",
    "### Decision Tree is one of the easiest and popular classification algorithms to understand and interpret. It can be utilized for both classification and regression kind of problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm\n",
    "\n",
    "\n",
    "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. \n",
    "\n",
    "\n",
    "The topmost node in a decision tree is known as the root node. \n",
    "\n",
    "\n",
    "It learns to partition on the basis of the attribute value. \n",
    "\n",
    "\n",
    "It partitions the tree in recursively manner call recursive partitioning. \n",
    "\n",
    "\n",
    "This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. \n",
    "\n",
    "\n",
    "That is why decision trees are easy to understand and interpret.\n",
    "\n",
    "<img src='img/dt1.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is a white box type of ML algorithm.\n",
    "\n",
    "\n",
    "It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network.\n",
    "\n",
    "\n",
    "Its training time is faster compared to the neural network algorithm. \n",
    "\n",
    "\n",
    "The time complexity of decision trees is a function of the number of records and number of attributes in the given data. \n",
    "\n",
    "\n",
    "The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. \n",
    "\n",
    "\n",
    "Decision trees can handle high dimensional data with good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the Decision Tree algorithm work?\n",
    "The basic idea behind any decision tree algorithm is as follows:\n",
    "\n",
    "1. Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n",
    "\n",
    "\n",
    "2. Make that attribute a decision node and breaks the dataset into smaller subsets.\n",
    "\n",
    "\n",
    "3. Starts tree building by repeating this process recursively for each child until one of the condition will match:\n",
    "\n",
    "\n",
    "    * All the tuples belong to the same attribute value.\n",
    "\n",
    "    * There are no more remaining attributes.\n",
    "\n",
    "    * There are no more instances.\n",
    "\n",
    "<img src='img/dt2.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute Selection Measures\n",
    "\n",
    "Attribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best possible manner. \n",
    "\n",
    "\n",
    "It is also known as splitting rules because it helps us to determine breakpoints for tuples on a given node. \n",
    "\n",
    "\n",
    "ASM provides a rank to each feature(or attribute) by explaining the given dataset. \n",
    "\n",
    "\n",
    "Best score attribute will be selected as a splitting attribute. \n",
    "\n",
    "\n",
    "In the case of a continuous-valued attribute, split points for branches also need to define. \n",
    "\n",
    "\n",
    "Most popular selection measures are Information Gain, Gain Ratio, and Gini Index.\n",
    "\n",
    "## Reference Material for more details\n",
    "http://www.ijoart.org/docs/Construction-of-Decision-Tree--Attribute-Selection-Measures.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "Shannon invented the concept of entropy, which measures the impurity of the input set. \n",
    "\n",
    "\n",
    "In physics and mathematics, entropy referred as the randomness or the impurity in the system. \n",
    "\n",
    "\n",
    "In information theory, it refers to the impurity in a group of examples. Information gain is the decrease in entropy. \n",
    "\n",
    "\n",
    "Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain.\n",
    "\n",
    "<img src='img/dt3.JPG'>\n",
    "\n",
    "* Info(D) is the average amount of information needed to identify the class label of a tuple in D.\n",
    "* |Dj|/|D| acts as the weight of the jth partition.\n",
    "* InfoA(D) is the expected informa-tion required to classify a tuple from D based on the partitioning by A.\n",
    "\n",
    "\n",
    "The attribute A with the highest information gain, Gain(A), is chosen as the splitting attribute at node N()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
