{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='1.JPG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Mart Dataset walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('Train_UWu5bXk.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=train.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(a))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There aren’t too many missing values (just 2 variables have them actually). \n",
    "\n",
    "#### We can impute the values using appropriate methods, or we can set a threshold of, say 20%, and remove the variable having more than 20% missing values. \n",
    "\n",
    "    Let’s look at how this can be done in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = train.columns\n",
    "variable=[]\n",
    "#print(type(variables))\n",
    "for i in range(0,12):\n",
    "    if a[i] <= 20:\n",
    "        variable.append(variables[i])\n",
    "        \n",
    "print(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values\n",
    "\n",
    "####  impute the missing values in the Item_Weight column using the median value of the known Item_Weight observations. For the Outlet_Size column, we will use the mode of the known Outlet_Size values to impute the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.Outlet_Size.unique())\n",
    "#print(train.Item_Weight.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlet_Size is categorical, so the imputation will be done using mode\n",
    "#### Item_weight is continuous, so the imputation will be done using median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Item_Weight'].fillna(train['Item_Weight'].median(),inplace=True)\n",
    "train['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking whether imputation has been successfully done or not\n",
    "\n",
    "#### mean should be zero for all null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now checking for Low Variance filter\n",
    "\n",
    "* Consider a variable in our dataset where all the observations have the same value, say 1. \n",
    "* If we use this variable, do you think it can improve the model we will build? \n",
    "* The answer is no, because this variable will have zero variance.\n",
    "\n",
    "### So, we need to calculate the variance of each numerical variable. \n",
    "Then drop the variables having low variance as compared to other variables in our dataset. The reason for doing this, as I mentioned above, is that variables with a low variance will not affect the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From above we can see that variance of Item_Visibility is very less as compared to the other variables. \n",
    "* We can safely drop this column. \n",
    "\n",
    "### Building a low variance filter with threshold of 10%:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = train[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']]\n",
    "var = numeric.var()\n",
    "numeric = numeric.columns\n",
    "variable = [ ]\n",
    "for i in range(0,len(var)):\n",
    "    if var[i]>=10:   #setting the threshold as 10%\n",
    "       variable.append(numeric[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Correlation or Multicollinearity Filter\n",
    "\n",
    "* High correlation between two variables means they have similar trends and are likely to carry similar information. \n",
    "* This can bring down the performance of some models drastically (linear and logistic regression models, for instance). \n",
    "* We can calculate the correlation between independent numerical variables that are numerical in nature. \n",
    "* If the correlation coefficient crosses a certain threshold value, we can drop one of the variables (dropping a variable is highly subjective and should always be done keeping the domain in mind).\n",
    "\n",
    "\n",
    "### Dropping dependent variable Item_Outlet_Sales and saving remaining variable in a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=train.drop('Item_Outlet_Sales',1)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that there are no variables having high correlation. If the correlation coefficient between 2 variables is between 0.5-0.6 we should seriously consider dropping one of these 2 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "* Random Forest is one of the most widely used algorithms for feature selection. \n",
    "* It comes packaged with in-built feature importance so you don’t need to program that separately. \n",
    "* This helps us select a smaller subset of features.\n",
    "\n",
    "* We need to convert the data into numeric form by applying one hot encoding, as Random Forest (Scikit-Learn Implementation) takes only numeric inputs. pd.get_dummies()\n",
    "* Dropping the ID variables (Item_Identifier and Outlet_Identifier) as these are just unique numbers and thus insignificant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "df=df.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)\n",
    "model = RandomForestRegressor(random_state=1, max_depth=10)\n",
    "df=pd.get_dummies(df) # Convert categorical variable into dummy/indicator variables. (One Hot Encoding)\n",
    "model.fit(df,train.Item_Outlet_Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After fitting the model, plot the feature importance graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = df.columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-9:]  # top 10 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='r', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on above graph we can pick the top-most features to reduce dimensionality of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Feature Elimination\n",
    "\n",
    "Follow the below steps to understand and use the ‘Backward Feature Elimination’ technique:\n",
    "\n",
    "* First take all the n variables present in our dataset and train the model using them\n",
    "* Then calculate the performance of the model\n",
    "* Compute the performance of the model after eliminating each variable (n times), i.e., we drop one variable every time and train the model on the remaining n-1 variables\n",
    "* Identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n",
    "* Repeat this process until no variable can be dropped\n",
    "\n",
    "### This technique is used while when building Linear or Logistic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import datasets\n",
    "lreg = LinearRegression()\n",
    "rfe = RFE(lreg, 10)\n",
    "rfe = rfe.fit(df, train.Item_Outlet_Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Need to specify the algorithm and number of features to select & get back the list of variables obtained from backward feature elimination. \n",
    "* To check the ranking of the variables use “rfe.ranking_” command. The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
